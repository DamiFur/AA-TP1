\documentclass[10pt,a4paper]{article}
\usepackage[a4paper,bindingoffset=0.2in,%
            left=1in,right=1in,top=1in,bottom=1in,%
            footskip=.25in]{geometry}

\usepackage[utf8x]{inputenc}
\usepackage[spanish]{babel}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{listingsutf8}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{epstopdf}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% set the default code style
\lstset{
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=4, % tab space width
    showstringspaces=false, % don't mark spaces in strings
    numbers=left, % display line numbers on the left
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red} % string color
}

% mathy stuff
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Demostración]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definición]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\title{Aprendizaje Automático \\ Trabajo Práctico 1 \\ Detección de Spam }

\newcommand{\order}[1]{$\mathcal{O}(#1)$}

\begin{document}

%% cover page

\maketitle

\bigskip

\begin{table}[h]
\centering
\begin{tabular}{|l l l l|}
\hline
Integrante       & \multicolumn{1}{c}{LU}     & Correo electrónico       	& Carrera \\ \hline
Martín Baigorria & \multicolumn{1}{c}{575/14} & martinbaigorria@gmail.com & Computación (Licenciatura) \\ 
Damián Furman & \multicolumn{1}{c}{936/11}& damian.a.furman@gmail.com & Computación (Licenciatura)\\
Germán Abrevaya & \multicolumn{1}{c}{-} & germanabrevaya@gmail.com & Física (Doctorado)\\ \hline
\end{tabular}
\end{table}

\vfill

\begin{center}
\textbf{Reservado para la cátedra}
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
Instancia       & Docente & Nota \\ \hline
Primera entrega &         &      \\ \hline
Segunda entrega &         &      \\ \hline
\end{tabular}
\end{table}

\newpage
\tableofcontents
\newpage

% end cover page

\section{Introducción}

El objetivo de este trabajo practico es experimentar con diferentes técnicas de aprendizaje automático para entrenar un clasificador de mensajes de correo electrónico en dos clases, 'spam' y 'ham'. Para ello, se utilizaran diferentes modelos y métodos para buscar los diferentes atributos que utilizaremos. Se experimentaran con diferentes técnicas de reducción de dimensionalidad. Además, se evaluaran los diferentes hiperparametros de cada modelo para intentar buscar una mejor performance de los algoritmos de aprendizaje. Dejamos el 10\% de los datos como testing set, utilizando el resto para experimentar con los diferentes modelos. Al elegir los hiperparametros sobre el training set, caemos en la posibilidad de estar haciendo overfitting. Siguiendo el Principio de Occam, a performance similar, optaremos siempre por el modelo mas simple.

\section{Extracción de atributos}

\subsection{Bag of words}

En una primera etapa, antes de intentar seleccionar atributos de forma manual, la idea fue analizar la frecuencia de cada palabra en cada mail, ignorando el orden de las palabras. Notar que los mails están en formato MIME, por lo que no se podía simplemente hacer un análisis de frecuencia sobre el MIME en plaintext, si no que había que utilizar un parser para luego poder analizar las diferentes partes de cada correo. Además, muchos de los correos no estaban en UTF-8, lo que dificultaba el análisis. Por esa razón decidimos ignorarlos para el análisis de frecuencia.

Dado que el alto tiempo de computo para calcular la frecuencia de cada palabra en cada mail, decidimos simplemente tomar una variable binaria por mail que indica si la misma esta o no en el mismo. La idea fue calcular por clase cuantas veces aparecía cada palabra en el cuerpo de un correo, para luego ordenar todas las palabras por la diferencia absoluta en frecuencia relativa y de esa forma seleccionar los mejores atributos. Seguramente existe un orden mejor, pero lo que buscábamos era encontrar las palabras que mas distinguían entre las dos clases. Una idea que teníamos a priori además es que todas las conjunciones iban a aparecer con la misma frecuencia en ambas clases, por lo que al calcular su respectivo score se compensarían y no aparecerían en nuestros resultados finales. Además ignoramos todos los números, dado que creemos que no es un buen predictor.

En una primera etapa, para decidir que atributos se iban a tener en cuenta tomamos todas las palabras que mostraban una diferencia de frecuencia relativa entre clases de mails mayor o igual a 500. De esta forma terminamos preseleccionando 1.019 atributos. Entre ellos se encuentran:

['subject', 'please', 'original message', 'thanks', 'will', 'have', 'this', 'know', 'that', 'for', 'let', 'enron', 'would', 'any', 'attached', 'questions', 'call', 'are', 'here', 'with', 'gas', 'date', 'corp', 'has', 'monday', 'file', 'friday', 'and', 'houston', 'october', 'wednesday', 'not', 'energy', 'you', 'meeting', 'tuesday', 'need', 'thursday', 'group', 'california', 'power', 'but', 'following', 'there', 'november', 'should', 'final', 'was', 'more', 'they', 'schedule', 'review', 'think', 'john', 'deal', 'ect', 'start', 'scheduling', 'portland', 'what', 'money', 'been', 'contract', 'fax', 'professional', 'may', 'mark', 'hou', 'last', 'work', 'contact', 'iso', 'issues', 'schedules', 'log', 'vince', 'viagra', 'still', 'however', 'hourahead', 'can', 'solicitation', 'back', 'between', 'comments', 'mailto', 'also', 'discuss', 'thank', 'software', 'january', 'messages', 'save', 'were', 'mike', 'your', 'received', 'site', 'had', 'changes', 'advertisement', 'size', 'could', 'smith', 'prices', 'txt', 'westdesk', 'parsing', 'morning', 'louise', 'david', 'sure', 'well', 'going', 'click', 'working', 'visit', 'font', 'only', 'jeff', 'trading', 'stop', 'next', 'like', 'plan', 'did', 'tomorrow', 'now', 'north', 'regarding', 'product', 'best', 'agreement', 'online', 'forward', 'conference', 'texas', 'fyi', 'prescription', 'products', 'point', 'family', 'process', 'these', 'team', 'world', 'adobe', 'results', 'kaminski', 'help', 'cialis', 'offers', 'note', 'market', 'management', 'many', 'employees', 'kevin', 'michael', 'scheduled', 'his', 'capacity', 'august', 'which', 'daily', 'bill', 'photoshop', 'end', 'data', 'request', 'dollars', 'probably', 'chris', 'james', 'below', 'two', 'act', 'him', 'microsoft', 'during', 'pipeline', 'future', 'how', 'ena', 'done', 'copies', 'prohibited', 'requested', 'delete', 'services', 'put', 'another', 'either', 'robert', 'macromedia', 'into', 'desk', 'able', 'weight', 'global', 'flash', 'related', 'sender', 'asked', 'jim', 'yourself', 'hope', 'markets', 'each', 'quality', 'distribution', 'kitchen', 'email', 'update', 'might', 'good', 'discussed', 'stocks', 'phone', 'our', 'set', 'differ', 'security', 'anything', 'offer', 'attachments', 'talk', 'wish', 'afternoon', 'privileged', 'yet', 'whether', 'create', 'look', 'low', 'easy', 'available', 'advice', 'uncertainties', 'summary', 'already', 'confidential', 'xls', 'meaning', '100\%', 'shipping', 'report', 'joe', 'copy', 'until', 'richard', 'cause', 'premiere', 'provide', 'west', 'risk']


\pagebreak

Por un lado unificamos las palabras 'original' y 'message', que en general muestran mensajes donde se hizo 'reply' o 'reply all'. Por otro lado, la palabra 'enron' aparecía bastante, lo que nos hizo pensar que el dataset que teníamos era el de Enron. Nos cuestionamos un poco que tan representativo es este dataset, dado que corresponde en general a mails corporativos. Notar que este mismo procedimiento se puede hacer para el título de los correos.

\subsection{Selección manual}

Luego de seleccionar los primeros atributos utilizando nuestra variacion de Bag of Words, decidimos elegir algunos otros atributos de forma manual. Estos fueron:

\begin{itemize}
	\item has\_closing\_tags
	\item has\_links
	\item cant\_capital
	\item capital\_in\_a\_row
	\item is\_multipart
\end{itemize}

Con estos atributos buscamos captar distintas propiedades frecuentes en los mails de spam: la presencia de links hacia páginas web de contenido malicioso o de spam, el formato HTML que otorga formato a cierta publicidad, la presencia de muchas palabras en mayúscula o títulos o subtítulos con varias letras con mayúsculas seguidas.

A su vez, a las palabras determinadas por el Bag of Words le agregamos varias palabras que consideramos de posible aparición frecuente en mails de spam, a saber: offer, sex, viagra, Nigeria, discount entre varias otras.

\section{Model Selection}

Luego de definir nuestros atributos, probamos distintos modelos con el objetivo de evaluar su performance. Decidimos utilizar para nuestra comparación Naive Bayes, Decision Tree Classifier, Random Forest Classifier, Support Vector Machine y K Nearest Neighbours y AdaBoost. 

%Éste último es similar a la técnica de boosting vista en clase, en la cual la salida de otros algoritmos de machine learning (``weak learners'') son combinadas en una suma pesada que representa la salida final del clasificador. La principal diferencia de un simple boosting respecto a AdaBoostes que este último es adaptativo en el sentido de que los weak learners subsiguientes son a su vez orientados para corregir las instancias mal clasificadas por los anteriores weak learners.

Antes de entrenar los modelos, se separó el 10\% de los datos totales para ser empleados como test set, y el resto como training set. Para seleccionar un modelo final, la idea es probar todos los modelos bajo diferentes configuraciones haciendo cross validation sobre el training set, y luego seguir trabajando con el que nos de los mejores resultados. El test set lo guardamos para cuando finalmente tengamos seleccionado un modelo, podamos tener una idea de que tan bien generaliza bajo la elección de hiperparametros que hagamos.

\subsection{Support Vector Machines}

SVM resultó poco practico debido al descomunal tiempo de computo necesario para entrenar el modelo. Quitándole 90 atributos calculados mediante Bag of Words el tiempo de cómputo para entrenar al modelo (sin cross-validation) fue de 2:17 horas. El test mean accuracy resultó en este caso de 0.7933, la precision fue de 0.8829 y el recall, 0.6739. SVM utiliza quadratic programming lo cual conlleva un gran tiempo de cálculo cuando se trabaja con muchas dimensiones. No hay un criterio marcado de localidad para buscar hiperplanos separadores. Para sacarle más provecho a este método haría falta un kernel especifico.

\pagebreak

\subsection{Naive Bayes}

A continuación se pueden ver los resultados de diferentes configuraciones de Naive Bayes con probabilidades gausianas. Dado el supuesto de independencia de los features, decidimos ir agregando atributos de a poco para observar como se comportaban las diferentes métricas de performance.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\textbf{}          & \multicolumn{1}{c|}{\textbf{200 Atributos}} & \multicolumn{1}{c|}{\textbf{500 Atributos}} & \multicolumn{1}{c|}{\textbf{Todos}} \\ \hline
Train mean accuracy & 0.6798 & 0.6851 & 0.6493 \\ \hline
False positives     & 2593 & 2680 & 3060 \\ \hline
False negatives     & 288 & 154 & 96 \\ \hline
True positives      & 4220 & 4354 & 4412 \\ \hline
True negatives      & 1899 & 1812 & 1432 \\ \hline
Precision           & 0.6194 & 0.6189 & 0.5904 \\ \hline
Recall              & 0.9361 & 0.9658 & 0.9787 \\ \hline
	\end{tabular}
	\caption{Resultados de Naive Bayes al ir agregando atributos al modelo. Los resultados son calculados tomando el promedio de cada métrica al hacer 10 fold cross validation.}
	\label{naive}
\end{table}

Lo primero que nos llamo la atención fue el bajo tiempo de computo (menor a 5 minutos) en comparación con SVM. Otro factor interesante que observamos es que agregar atributos no necesariamente es mejor. La ganancia en accuracy en pasar de 200 a 500 atributos no parece significativa, y de hecho baja cuando agregamos todos los atributos que teníamos disponibles. En este caso parece que los mejores resultados los logramos con 500 atributos. En caso de que los otros modelos no logren una diferencia significativa en performance, quizás puede ser interesante seguir probando con Naive Bayes y técnicas de feature selection.

\subsection{Random Forests}

A continuación experimentamos con la técnica de Random Forests, variando el hiperparametro que define la cantidad de arboles utilizados en en el ensemble y utilizando todos los atributos disponibles.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{}} & \multicolumn{1}{c|}{\textbf{30 árboles}} & \multicolumn{1}{c|}{\textbf{50 árboles}} & \multicolumn{1}{c|}{\textbf{100 árboles}} & \textbf{150 árboles} & \textbf{200 árboles} \\
\hline
Train mean accuracy & 0.9795 & 0.9786 & 0.9812 & 0.9801 & 0.9808\\ \hline
False positives     & 82 & 94 & 83 & 93 & 88\\ \hline
False negatives     & 102 & 98 & 96 & 86 & 84\\ \hline
True positives      & 4406 & 4410 & 4422 & 4422 & 4424\\ \hline
True negatives      & 4410 & 4398 & 4409 & 4399 & 4404       \\ \hline
Precision           & 0.9817 & 0.9791 & 0.9810 & 0.9796 & 0.9813 \\ \hline
Recall              & 0.9773 & 0.9767 & 0.9780 & 0.9802 & 0.9804 \\ \hline
Time taken          & 15.84s & 23.69s & 46.27s & 66.67s & 89.44s \\ \hline
	\end{tabular}
	\caption{Resultados de Random Forest cambiando la cantidad de arboles. Los resultados son calculados tomando el promedio de cada métrica al hacer 10 fold cross validation.}
	\label{rf}
\end{table}

En general el tiempo de computo fue menor a 2 minutos, hasta en el caso en el que utilizamos 200 arboles. Parece que el tiempo es lineal en la cantidad de arboles. En cuanto a performance, Random Forests con 100 arboles tiene la mejor accuracy y precision, por lo que hasta el momento es el modelo que utilizaremos.

Se puede observar que aumentar la cantidad de árboles en general tiende a mejorar la \textit{accuracy} aunque su variación es relativamente pequeña (alrededor del 0,2\%). Sin embargo, a partir de los 100 arboles la \textit{accuracy} comienza a disminuir, aunque también con una tasa de decrecimiento baja. En general, a partir de los 100 arboles los resultados tampoco parecen cambiar mucho, por lo que optar por el modelo mas simple parece la mejor opción.

\pagebreak

\subsection{kNN:}

Para K Nearest Neigbours consideramos intentar mejorar los resultados a partir de variar los k vecinos considerados. Mientras que en una primera instancia utilizamos un k = 5, testeamos luego los resultados de aplicar un k = 10 y k = 20.

\begin{table}[H]
	\centering
	\label{my-label}
	\begin{tabular}{|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{}} & \multicolumn{1}{c|}{\textbf{K = 5}} & \multicolumn{1}{c|}{\textbf{K = 10}} & \multicolumn{1}{c|}{\textbf{K = 20}} \\ \hline
Train mean accuracy & 0.8244 & 0.8141 & 0.7994 \\ \hline
False positives     & 821 & 708 & 792 \\ \hline
False negatives     & 759 & 965 & 1013 \\ \hline
True positives      & 3749 & 3543 & 3495 \\ \hline
True negatives      & 3671 & 3784 & 3700 \\ \hline
Precision           & 0.8203 & 0.8127 & 0.8152 \\ \hline
Recall              & 0.8316 & 0.8334 & 0.7752 \\ \hline
	\end{tabular}
	\caption{Resultados de k Nearest Neighbours cambiando la cantidad de vecinos utilizados para clasificar una nueva instancia. Los resultados son calculados tomando el promedio de cada métrica al hacer 10 fold cross validation.}
	\label{rf}
\end{table}

En cuanto a tiempo de computo, los tres modelos tomaron menos de 10 segundos. Podemos observar que en la medida que aumenta la cantidad de vecinos que consideramos al clasificar una nueva instancia k, los resultados en general empeoran. Esto se debe a que al clasificar una nueva instancia, estamos considerando puntos que probablemente no son locales. Dados estos resultados, Random Forests con 100 arboles sigue siendo el que utilizaremos.

\section{Reducción de dimensionalidad}

Se probó el uso de PCA como técnica de reducción de dimensionaidad y algunos selectores de features, basados en test estadísticos univariados. Para ambos casos se vio que la eficiencia, en cuanto a accuracy, precision y recall, disminuía sin importar con cuantos features nos quedásemos. Por lo tanto decidimos simplemente utilizar todos los features que teníamos disponibles.

\section{Modelo final y performance}

Finalmente, dada nuestra experimentación decidimos quedarnos con el modelo que utiliza Random Forests con 100 arboles, sin reducción de dimensionalidad. A continuación podemos ver los resultados que tuvimos entrenando sobre el training set entero y calculando las métricas sobre el test set, comparados con los resultados que obtuvimos sobre el training set.

\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Validacion}} & \multicolumn{1}{c|}{\textbf{Cross validation}} & \multicolumn{1}{c|}{\textbf{Test set}} \\
\hline
Train mean accuracy & 0.9812 & 0.9795\\ \hline
False positives     & 83 & 86 \\ \hline
False negatives     & 96 & 98\\ \hline
True positives      & 4422 & 4410\\ \hline
True negatives      & 4409 & 4406       \\ \hline
Precision           & 0.9810 & 0.9808 \\ \hline
Recall              & 0.9780 & 0.9782 \\ \hline
Time taken          & 46.27s & 0.9782 \\ \hline
	\end{tabular}
	\caption{Performance de Random Forests. En la primera columna, la performance se mide tomando el promedio de cada métrica al hacer 10 fold cross validation. Por otro lado, la segunda columna muestra el modelo entrenado sobre el traning set y validado sobre el test set.}
	\label{rfperf}
\end{table}

Por lo que podemos ver, en general el modelo parece haber generalizo bien y mantenido su performance.

% \section{Resultados}
% Describir los resultados conseguidos por los distintos modelos y conjuntos de atributos considerados. Preferentemente, resumir los resultados en tablas/figuras. Mencionar los tiempos de ejecución aproximados de cada técnica.

\pagebreak

\section{Discusión}

Luego de experimentar con diferentes técnicas de aprendizaje, observamos que los mejores resultados se obtuvieron utilizando métodos basados en arboles, en particular uno con 100 arboles. En el caso del spam, estamos especialmente interesados en disminuir la cantidad de falsos positivos. Random Forests tuvo el menor numero de falsos positivos comparado con otros métodos. El tiempo de ejecución fue otro factor que consideramos relevante. A diferencia de otros métodos como SVM, en general Random Forests fue relativamente rápido de computar.

Son interesantes algunas de las estrategias que emplean los spammers para evitar este tipo de filtros, como por ejemplo \emph{bayesian poisoning}, en la cual procuran emplear la información con la que se entrenan a los filtros de spam para disminuir su efectividad. Incluyen en sus mails spam palabras y otros atributos estadísticamente característicos de mails ham con el fin de aumentar las probabilidades de ser clasificados como ham. Por otro lado, si los filtros son entrenados con estos mails spam 'envenenados', tenderá a aumentar la cantidad de falsos positivos.

%En principio los mo
%
%
%%https://en.wikipedia.org/wiki/Bayesian_poisoning
%
%%https://www.microsoft.com/en-us/research/publication/why-do-nigerian-scammers-say-they-are-from-nigeria/
%
%Como una primera conclusión de este experimento observamos que los mejores resultados se obtienen con los modelos más 'sencillos', mientras que los modelos que requieren mayor cómputo como kNN o SVM arrojan peores resultados.
%Los métodos basádos en árboles, y en particular Random Forest, además de otorgar los mejores resultados, disminuyen especialmente la cantidad de falsos positivos (mejoran la métrica \textit{precision}), propiedad que nos interesa destacar al armar un clasificador de Spam
%
%La conclusión de esta experiencia, sin embargo, parecería ser que la cantidad de árboles del Random Forest no es un factor determinante a la hora de evaluar su eficiencia.

% Analizar los resultados, buscando responder cuestiones como, por ejemplo: ¿cuáles son los atributos encontrados con mayor poder predictivo?, ¿cuán sensibles fueron los algoritmos a las técnicas de reducción de dimensionalidad consideradas?, ¿resultó clara la elección del algoritmo para la competencia, o hubo que poner en la balanza distintos factores?


\end{document}