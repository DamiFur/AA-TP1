\documentclass[10pt,a4paper]{article}
\usepackage[paper=a4paper]{geometry}

\usepackage[utf8x]{inputenc}
\usepackage[spanish]{babel}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{listingsutf8}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{epstopdf}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% set the default code style
\lstset{
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=4, % tab space width
    showstringspaces=false, % don't mark spaces in strings
    numbers=left, % display line numbers on the left
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red} % string color
}

% mathy stuff
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Demostración]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definición]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\title{Aprendizaje Automatico \\ Trabajo Práctico 1 \\ Detección de Spam }

\newcommand{\order}[1]{$\mathcal{O}(#1)$}

\begin{document}

%% cover page

\maketitle

\bigskip

\begin{table}[h]
\centering
\begin{tabular}{|l l l|}
\hline
Integrante       & \multicolumn{1}{c}{LU}     & Correo electrónico        \\ \hline
Martin Baigorria & \multicolumn{1}{c}{575/14} & martinbaigorria@gmail.com \\ 
Damián Furman & 	936/11                      & z \\
Germán Abrevaya & y                      & x \\ \hline
\end{tabular}
\end{table}

\vfill

\begin{center}
\textbf{Reservado para la cátedra}
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
Instancia       & Docente & Nota \\ \hline
Primera entrega &         &      \\ \hline
Segunda entrega &         &      \\ \hline
\end{tabular}
\end{table}

\newpage
\tableofcontents
\newpage

% end cover page

\section{Introducción}

El objetivo de este trabajo practico es experimentar con diferentes técnicas de aprendizaje automático para entrenar un clasificador de mensajes de correo electrónico en dos clases, 'spam' y 'ham'. Para ello, se utilizaran diferentes modelos y métodos para buscar los diferentes atributos que utilizaremos. Se experimentaran con diferentes técnicas de reducción de dimensionalidad. Además, se evaluaran los diferentes hiperparametros de cada modelo para poder buscar alguna forma de evitar hacer overfitting de los datos. Dejamos el 10\% de los datos como testing set, utilizando el resto para experimentar con los diferentes modelos.

\section{Extracción de atributos}

\subsection{Bag of words}

En una primera etapa, antes de intentar seleccionar atributos de forma manual, la idea fue analizar la frecuencia de cada palabra en cada mail, ignorando el orden de las palabras. Notar que los mails están en formato MIME, por lo que no se podía simplemente hacer un análisis de frecuencia sobre el MIME en plaintext, si no que había que utilizar un parser para luego poder analizar las diferentes partes de cada correo. Ademas, muchos de los correos no estaban en utf-8, lo que dificultaba el analisis. Por esa razón decidimos ignorarlos para el análisis de frecuencia.

Dado que el alto tiempo de computo para calcular la frecuencia de cada palabra en cada mail, decidimos simplemente tomar una variable binaria por mail que indica si la misma esta o no en el mismo. La idea fue calcular por clase cuantas veces aparecía cada palabra en el cuerpo de un correo, para luego ordenar todas las palabras por la diferencia absoluta en frecuencia relativa y de esa forma seleccionar los mejores atributos. Seguramente existe un orden mejor, pero lo que buscábamos era encontrar las palabras que mas distinguían entre las dos clases. Una idea que teníamos a priori además es que todas las conjunciones iban a aparecer con la misma frecuencia en ambas clases, por lo que al calcular el score se compensarían y no aparecerían en nuestros resultados finales. Además ignoramos todos los números, dado que creemos que no es un buen predictor.

En una primera instancia, decidimos tomar las mejores 200 palabras ordenadas por nuestro score para luego intentar seleccionar un subconjunto.

Por un lado creemos que todas las palabras que son sustantivos propios referidos a meses o nombres de personas no son buenos predictores. Lo mismo sucede con las conjunciones que finalmente terminaron apareciendo. De esta manera terminamos con el siguiente conjunto de palabras:

['please', 'original message', 'thanks', 'any', 'attached', 'questions', 'call', 'gas', 'date', 'corp', 'file',
	'energy', 'need', 'meeting', 'group', 'power', 'following', 'there', 'final', 'should', 'more', 'schedule',
	'review', 'think', 'week', 'some', 'deal', 'start', 'scheduling', 'contract', 'money', 'professional', 'been',
	'last', 'work', 'schedules', 'issues', 'viagra', 'however', 'contact', 'thank', 'between', 'solicitation', 'comments',
	'sex', 'messages', 'discuss', 'software', 'save', 'received', 'site', 'changes', 'txt', 'advertisement', 'parsing', 'prices',
	'morning', 'click', 'sure', 'visit', 'stop', 'only', 'working', 'next', 'trading', 'plan', 'tomorrow',
	'awarded', 'soft', 'detected', 'now', 'like', 'about', 'doc', 'who', 'windows', 'basis', 'online', 'product', 'conference',
	'prescription', 'products', 'best', 'fyi', 'point', 'agreement', 'regarding', 'forward', 'north', 'family', 'world', 'team',
	'process', 'help', 'cialis', 'adobe', 'down', 'results', 'thousand', 'first', 'issue', 'link', 'offers', 'note',
	'scheduled', 'management', 'capacity', 'market', 'bill', 'employees', 'daily', 'dollars']

\pagebreak

Por un lado unificamos las palabras 'original' y 'message', que en general muestran mensajes donde se hizo 'reply' o 'reply all'. Por otro lado, la palabra 'enron' aparecía bastante, lo que nos hizo pensar que el dataset que teníamos era el de Enron. Nos cuestionamos un poco que tan representativo es este dataset, dado que corresponde en general a mails corporativos. Notar que este mismo procedimiento se puede hacer para el titulo de los correos.

\subsection{Selección manual}

Luego de seleccionar los primeros atributos utilizando nuestra variacion de Bag of Words, decidimos elegir algunos otros atributos de forma manual. Estos fueron:

\begin{itemize}
	\item has\_closing\_tags
	\item has\_links
	\item cant\_capital
	\item capital_in_a_row
	\item is\_multipart
\end{itemize}

Con estos atributos buscamos captar distintas propiedades frecuentes en los mails de spam: la presencia de links hacia páginas web de contenido malicioso o de spam, el formato HTML que otorga formato a cierta publicidad, la presencia de muchas palabras en mayúscula o títulos o subtítulos con varias letras con mayúsculas seguidas

A su vez, a las palabras determinadas por el Bag of Words le agregamos varias palabras que consideramos de posible aparición frecuente en mails de spam, a saber: offer, sex, viagra, Nigeria, discount entre varias otras

\subsection{Otras alternativas}

Utilizar NLP.

\section{Modelos}

Luego de definir nuestros atributos, probamos distintos modelos con el objetivo de evaluar su performance. Decidimos utilizar para nuestra comparación Naive Bayes, Decision Tree Classifier, Random Forest Classifier, Support Vector Machine y K Nearest Neighbours. Medimos para cada uno el tiempo de cómputo, \textit{accuracy}, \textit{precision} y \textit{recall}, además de calcular la cantidad de falsos y verdaderos casos positivos y negativos. Los resultados son los siguientes:

\textbf{Random Forest Classifier:}
\begin{itemize}
	\item Test mean accuracy: 0.953777777778
	\item false positives: 169
	\item false negatives: 247
	\item true positives: 4187
	\item true negatives: 4397
	\item precision: 0.9612029384756657
	\item recall: 0.9442940911141182
	\item tiempo de cómputo: 272.41 segundos
\end{itemize}

\textbf{Decision Tree Classifier:}
\begin{itemize}
	\item Test mean accuracy: 0.920444444444
	\item false positives: 354
	\item false negatives: 362
	\item true positives:  4086
	\item true negatives:  4198
	\item precision: 0.9202702702702703
	\item recall: 0.9186151079136691
	\item tiempo de cómputo: 34.48 segundos
\end{itemize}

\textbf{Support Vector Machine:}
\begin{itemize}
	\item Test mean accuracy: 0.793333333333
	\item false positives: 400
	\item false negatives: 1460
	\item true positives:  3017
	\item true negatives:  4123
	\item precision: 0.8829382499268364
	\item recall: 0.6738887647978558
	\item tiempo de cómputo: 2:17:00 hs
\end{itemize}

\textbf{Naive Bayes with Gaussian Probabilities:}
\begin{itemize}
	\item Test mean accuracy: 0.805888888889
	\item false positives: 1321
	\item false negatives: 426
	\item true positives:  4100
	\item true negatives:  3153
	\item precision: 0.7563180225050729
	\item recall: 0.9058771542200619
	\item tiempo de cómputo: 0.32 segundos
\end{itemize}

\textbf{kNN:}
\begin{itemize}
	\item Test mean accuracy: 0.871444444444
	\item false positives: 762
	\item false negatives: 395
	\item true positives:  4131
	\item true negatives:  3712
	\item precision: 0.8442673206621705
	\item recall: 0.912726469288555
	\item tiempo de cómputo: 1181.95 segundos
\end{itemize}


Donde 'Test mean accuracy' es el promedio calculado a partir de la K-Folding Cross Validation con K = 10.

A su vez, vale considerar que para correr SVM quitamos 90 parámetros calculados mediante Bag of Words debido a que la complejidad del algoritmo depende de éstos y al correr este clasificador con la misma cantidad de parámetros con los que corrimos los otros modelos el tiempo de cómputo se volvía inmenso, lo cual lo descartaba en la práctica como un método de clasificación útil

Como una primera conclusión de este experimento observamos que los mejores resultados se obtienen con los modelos más 'sencillos', mientras que los modelos que requieren mayor cómputo como kNN o SVM arrojan peores resultados.
Los métodos basádos en árboles, y en particular Random Forest, además de otorgar los mejores resultados, disminuyen especialmente la cantidad de falsos positivos, propiedad que nos interesa destacar al armar un clasificador de Spam

% Listar los algoritmos de aprendizaje elegidos para experimentar. Describir cualquier decisión que hayan tomado (p.ej., elección de hiperparámetros).

% Discutir por que pensamos que no tiene sentido hacer SVM o kNN.

\subsection{Logistic Regression}
\subsection{Naive Bayes}
\subsection{Decision Tree Classifier}
\subsection{Random Forests}
\subsection{Bagging}
\subsection{Stacking}

\section{Reducción de dimensionalidad}

Describir brevemente las técnicas empleadas. PCA y penalidades.

\section{Resultados}
Describir los resultados conseguidos por los distintos modelos y conjuntos de atributos considerados. Preferentemente, resumir los resultados en tablas/figuras. Mencionar los tiempos de ejecución aproximados de cada técnica.


\section{Discusión}

Analizar los resultados, buscando responder cuestiones como, por ejemplo: ¿cuáles son los atributos encontrados con mayor poder predictivo?, ¿cuán sensibles fueron los algoritmos a las técnicas de reducción de dimensionalidad consideradas?, ¿resultó clara la elección del algoritmo para la competencia, o hubo que poner en la balanza distintos factores?


\end{document}