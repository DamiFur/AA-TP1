\documentclass[10pt,a4paper]{article}
\usepackage[paper=a4paper]{geometry}

\usepackage[utf8x]{inputenc}
\usepackage[spanish]{babel}

\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{xcolor}
\usepackage{listingsutf8}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{epstopdf}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% set the default code style
\lstset{
    frame=tb, % draw a frame at the top and bottom of the code block
    tabsize=4, % tab space width
    showstringspaces=false, % don't mark spaces in strings
    numbers=left, % display line numbers on the left
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red} % string color
}

% mathy stuff
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposición}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Demostración]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definición]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\title{Aprendizaje Automatico \\ Trabajo Práctico 1 \\ Detección de Spam }

\newcommand{\order}[1]{$\mathcal{O}(#1)$}

\begin{document}

%% cover page

\maketitle

\bigskip

\begin{table}[h]
\centering
\begin{tabular}{|l l l|}
\hline
Integrante       & \multicolumn{1}{c}{LU}     & Correo electrónico        \\ \hline
Martin Baigorria & \multicolumn{1}{c}{575/14} & martinbaigorria@gmail.com \\ 
Damián Furman & 	936/11                      & damian.a.furman@gmail.com \\
Germán Abrevaya & y                      & x \\ \hline
\end{tabular}
\end{table}

\vfill

\begin{center}
\textbf{Reservado para la cátedra}
\end{center}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
Instancia       & Docente & Nota \\ \hline
Primera entrega &         &      \\ \hline
Segunda entrega &         &      \\ \hline
\end{tabular}
\end{table}

\newpage
\tableofcontents
\newpage

% end cover page

\section{Introducción}

El objetivo de este trabajo practico es experimentar con diferentes técnicas de aprendizaje automático para entrenar un clasificador de mensajes de correo electrónico en dos clases, 'spam' y 'ham'. Para ello, se utilizaran diferentes modelos y métodos para buscar los diferentes atributos que utilizaremos. Se experimentaran con diferentes técnicas de reducción de dimensionalidad. Además, se evaluaran los diferentes hiperparametros de cada modelo para poder buscar alguna forma de evitar hacer overfitting de los datos. Dejamos el 10\% de los datos como testing set, utilizando el resto para experimentar con los diferentes modelos.

\section{Extracción de atributos}

\subsection{Bag of words}

En una primera etapa, antes de intentar seleccionar atributos de forma manual, la idea fue analizar la frecuencia de cada palabra en cada mail, ignorando el orden de las palabras. Notar que los mails están en formato MIME, por lo que no se podía simplemente hacer un análisis de frecuencia sobre el MIME en plaintext, si no que había que utilizar un parser para luego poder analizar las diferentes partes de cada correo. Ademas, muchos de los correos no estaban en utf-8, lo que dificultaba el analisis. Por esa razón decidimos ignorarlos para el análisis de frecuencia.

Dado que el alto tiempo de computo para calcular la frecuencia de cada palabra en cada mail, decidimos simplemente tomar una variable binaria por mail que indica si la misma esta o no en el mismo. La idea fue calcular por clase cuantas veces aparecía cada palabra en el cuerpo de un correo, para luego ordenar todas las palabras por la diferencia absoluta en frecuencia relativa y de esa forma seleccionar los mejores atributos. Seguramente existe un orden mejor, pero lo que buscábamos era encontrar las palabras que mas distinguían entre las dos clases. Una idea que teníamos a priori además es que todas las conjunciones iban a aparecer con la misma frecuencia en ambas clases, por lo que al calcular el score se compensarían y no aparecerían en nuestros resultados finales. Además ignoramos todos los números, dado que creemos que no es un buen predictor.

En una primera instancia, decidimos tomar las mejores 200 palabras ordenadas por nuestro score para luego intentar seleccionar un subconjunto.

Por un lado creemos que todas las palabras que son sustantivos propios referidos a meses o nombres de personas no son buenos predictores. Lo mismo sucede con las conjunciones que finalmente terminaron apareciendo. De esta manera terminamos con el siguiente conjunto de palabras:

['please', 'original message', 'thanks', 'any', 'attached', 'questions', 'call', 'gas', 'date', 'corp', 'file',
	'energy', 'need', 'meeting', 'group', 'power', 'following', 'there', 'final', 'should', 'more', 'schedule',
	'review', 'think', 'week', 'some', 'deal', 'start', 'scheduling', 'contract', 'money', 'professional', 'been',
	'last', 'work', 'schedules', 'issues', 'viagra', 'however', 'contact', 'thank', 'between', 'solicitation', 'comments',
	'sex', 'messages', 'discuss', 'software', 'save', 'received', 'site', 'changes', 'txt', 'advertisement', 'parsing', 'prices',
	'morning', 'click', 'sure', 'visit', 'stop', 'only', 'working', 'next', 'trading', 'plan', 'tomorrow',
	'awarded', 'soft', 'detected', 'now', 'like', 'about', 'doc', 'who', 'windows', 'basis', 'online', 'product', 'conference',
	'prescription', 'products', 'best', 'fyi', 'point', 'agreement', 'regarding', 'forward', 'north', 'family', 'world', 'team',
	'process', 'help', 'cialis', 'adobe', 'down', 'results', 'thousand', 'first', 'issue', 'link', 'offers', 'note',
	'scheduled', 'management', 'capacity', 'market', 'bill', 'employees', 'daily', 'dollars']

\pagebreak

Por un lado unificamos las palabras 'original' y 'message', que en general muestran mensajes donde se hizo 'reply' o 'reply all'. Por otro lado, la palabra 'enron' aparecía bastante, lo que nos hizo pensar que el dataset que teníamos era el de Enron. Nos cuestionamos un poco que tan representativo es este dataset, dado que corresponde en general a mails corporativos. Notar que este mismo procedimiento se puede hacer para el titulo de los correos.

\subsection{Selección manual}

Luego de seleccionar los primeros atributos utilizando nuestra variacion de Bag of Words, decidimos elegir algunos otros atributos de forma manual. Estos fueron:

\begin{itemize}
	\item has\_closing\_tags
	\item has\_links
	\item cant\_capital
	\item capital\_in\_a\_row
	\item is\_multipart
\end{itemize}

Con estos atributos buscamos captar distintas propiedades frecuentes en los mails de spam: la presencia de links hacia páginas web de contenido malicioso o de spam, el formato HTML que otorga formato a cierta publicidad, la presencia de muchas palabras en mayúscula o títulos o subtítulos con varias letras con mayúsculas seguidas

A su vez, a las palabras determinadas por el Bag of Words le agregamos varias palabras que consideramos de posible aparición frecuente en mails de spam, a saber: offer, sex, viagra, Nigeria, discount entre varias otras

\subsection{Otras alternativas}

Utilizar NLP.

\section{Modelos}

Luego de definir nuestros atributos, probamos distintos modelos con el objetivo de evaluar su performance. Decidimos utilizar para nuestra comparación Naive Bayes, Decision Tree Classifier, Random Forest Classifier, Support Vector Machine y K Nearest Neighbours. Medimos para cada uno el tiempo de cómputo, \textit{accuracy}, \textit{precision} y \textit{recall}, además de calcular la cantidad de falsos y verdaderos casos positivos y negativos. Los resultados son los siguientes:

\textbf{Random Forest Classifier:}
\begin{itemize}
	\item Test mean accuracy: 0.953777777778
	\item false positives: 169
	\item false negatives: 247
	\item true positives: 4187
	\item true negatives: 4397
	\item precision: 0.9612029384756657
	\item recall: 0.9442940911141182
	\item tiempo de cómputo: 272.41 segundos
\end{itemize}

\textbf{Decision Tree Classifier:}
\begin{itemize}
	\item Test mean accuracy: 0.920444444444
	\item false positives: 354
	\item false negatives: 362
	\item true positives:  4086
	\item true negatives:  4198
	\item precision: 0.9202702702702703
	\item recall: 0.9186151079136691
	\item tiempo de cómputo: 34.48 segundos
\end{itemize}

\textbf{Support Vector Machine:}
\begin{itemize}
	\item Test mean accuracy: 0.793333333333
	\item false positives: 400
	\item false negatives: 1460
	\item true positives:  3017
	\item true negatives:  4123
	\item precision: 0.8829382499268364
	\item recall: 0.6738887647978558
	\item tiempo de cómputo: 2:17:00 hs
\end{itemize}

\textbf{Naive Bayes with Gaussian Probabilities:}
\begin{itemize}
	\item Test mean accuracy: 0.805888888889
	\item false positives: 1321
	\item false negatives: 426
	\item true positives:  4100
	\item true negatives:  3153
	\item precision: 0.7563180225050729
	\item recall: 0.9058771542200619
	\item tiempo de cómputo: 0.32 segundos
\end{itemize}

\textbf{kNN:}
\begin{itemize}
	\item Test mean accuracy: 0.871444444444
	\item false positives: 762
	\item false negatives: 395
	\item true positives:  4131
	\item true negatives:  3712
	\item precision: 0.8442673206621705
	\item recall: 0.912726469288555
	\item tiempo de cómputo: 1181.95 segundos
\end{itemize}


Donde 'Test mean accuracy' es el promedio calculado a partir de la K-Folding Cross Validation con K = 10.

A su vez, vale considerar que para correr SVM quitamos 90 parámetros calculados mediante Bag of Words debido a que la complejidad del algoritmo depende de éstos y al correr este clasificador con la misma cantidad de parámetros con los que corrimos los otros modelos el tiempo de cómputo se volvía inmenso, lo cual lo descartaba en la práctica como un método de clasificación útil

Como una primera conclusión de este experimento observamos que los mejores resultados se obtienen con los modelos más 'sencillos', mientras que los modelos que requieren mayor cómputo como kNN o SVM arrojan peores resultados.
Los métodos basádos en árboles, y en particular Random Forest, además de otorgar los mejores resultados, disminuyen especialmente la cantidad de falsos positivos (mejoran la métrica \textit{precision}), propiedad que nos interesa destacar al armar un clasificador de Spam

\subsection{Naive Bayes}

Naive Bayes tiene la interesante propiedad de dar resultados acpetables pagando un tiempo de cómputo mínimo (tarda 0.32 segundos en clasificar el corpus de texto completo compuesto por 9 mil mails). Sin embargo, sus resultados no son aceptables en términos de un clasificador real. Principalmente porque si bien logra un \textit{accuracy} de un 80\%, genera una gran cantidad de falsos positivos en relación a los falsos negativos (\textit{recall} es de un 90\% mientras que \textit{precision} es de un 75\%). Naive Bayes considera cada uno de los atributos como un factor independiente de los otros atributos que aporta 'probabilisticamente' a la decisión sobre la clasificación de un elemento. Por esto decidimos quitar varios de sus atributos e ir agregándoselos por partes para de esta manera comprobar cómo varían los resultados obtenidos.

\textbf{10 Atributos:}
\begin{itemize}
	\item Test mean accuracy: 0.49
	\item false positives: 4465
	\item false negatives: 125
	\item true positives:  4266
	\item true negatives:  144
	\item precision: 0.4886038254495476
	\item recall: 0.9715326804828057
\end{itemize}

\textbf{65 Atributos:}
\begin{itemize}
	\item Test mean accuracy: 0.616222222222
	\item false positives: 3269
	\item false negatives: 185
	\item true positives:  4285
	\item true negatives:  1261
	\item precision: 0.5672491395287265
	\item recall: 0.9586129753914989
\end{itemize}
\textbf{130 Atributos:}
\begin{itemize}
	\item Test mean accuracy: 0.805888888889
	\item false positives: 1321
	\item false negatives: 426
	\item true positives:  4100
	\item true negatives:  3153
	\item precision: 0.7563180225050729
	\item recall: 0.9058771542200619
\end{itemize}

Lo primero que podemos observar es que el clasificador Naive Bayes es muy sensible a la cantidad de atributos: parecería mejorar invariablemente en la medida que se agregan más. Esto sucede debido a que como cada atributo se considera independiente del resto y aporta una 'porción' de la probabilidad, al agregar más atributos la probabilidad total debería estar más afinada. Por otro lado, Naive Bayes no distingue distintas importancias entre los atributos, por lo que la efectividad de éste método también está supeditada a que la elección de los atributos que se agregan sea correcta.

Otra observación que puede realizarse es que cuando el clasificador no tiene suficiente cantidad de atributos tiende a clasificar todo en la misma categoría. En el caso de nuestra experiencia puede verse como con 10 atributos casi todos los mails fueron clasificados como Spam, lo cual genera que la métrica de \textit{precisión} sea bajísima mientras que el \textit{recall} tiene valores muy altos.


\subsection{Random Forests}

Debido a nuestros primeros resultados obtenidos al comparar distintos modelos, decidimos realizar varias pruebas con el clasificador Random Forest variando uno de sus hiperparámetros más importantes: la cantidad de árboles a utilizar. Mientras que en nuestra primera experiencia utilizamos 100 árboles, decidimos observar los resultados de aplicar el clasificador al mismo corpus pero con 50, 30, 150 y 200 árboles para observar cómo afectan los resultados en términos de predicción.

\textbf{30 Árboles:}
\begin{itemize}
	\item Test mean accuracy: 0.951777777778
	\item false positives: 150
	\item false negatives: 284
	\item true positives:  4256
	\item true negatives:  4310
	\item precision: 0.9659555152065366
	\item recall: 0.9374449339207048
\end{itemize}

\textbf{50 Árboles:}
\begin{itemize}
	\item Test mean accuracy: 0.952111111111
	\item false positives: 155
	\item false negatives: 276
	\item true positives:  4296
	\item true negatives:  4123
	\item precision: 0.9651763648618288
	\item recall: 0.9396325459317585
\end{itemize}

\textbf{150 Árboles:}
\begin{itemize}
	\item Test mean accuracy: 0.956555555556
	\item false positives: 144
	\item false negatives: 247
	\item true positives:  4256
	\item true negatives:  4353
	\item precision: 0.9672727272727273
	\item recall: 0.9451476793248945
\end{itemize}

\textbf{200 Árboles}
\begin{itemize}
	\item Test mean accuracy: 0.952888888889
	\item false positives: 158
	\item false negatives: 266
	\item true positives:  4247
	\item true negatives:  4329
	\item precision: 0.9641316685584563
	\item recall: 0.9410591624196765
\end{itemize}

En primer lugar se puede observar que disminuir la cantidad de árboles empeora la \textit{accuracy} aunque su variación es realmente mínima (0,2\%). Aumentar la cantidad de árboles, por otro lado, la aumenta, aunque también la diferencia es despreciable (0,2\% también). El principal efecto que parece generar tanto si reducimos la cantidad de árboles como si los aumentamos es ampliar la brecha entre \textit{precision} y \textit{recall}, aumentando un poco más significativamente (0,5\%) la \textit{precision}. Es decir, disminuye la cantidad de falsos positivos. Este cambio mejora el clasificador en términos de un clasificador de Spam.
A su vez, casi no se observan diferencias entre los resultados obtenidos con 150 y 200 árboles ni tampoco entre los resultados obtenidos con 30 o 50. Aún así, el hiperparámetro que mejor resultado obtuvo es al correr Random Forest con 150 árboles el cuál minimiza la cantidad de falsos positivos.
La conclusión de esta experiencia, sin embargo, parecería ser que la cantida de árboles del Random Forest no es un factor determinante a la hora de evaluar su eficiencia.


\subsection{kNN:}

Para K Nearest Neigbours consideramos intentar mejorar los resultados a partir de variar los k vecinos considerados. Mientras que en nuestra experiencia utilizamos un k = 5, testeamos los resultados de aplicar un k = 10 y k = 20

\textbf{K = 5:}
\begin{itemize}
	\item Test mean accuracy: 0.871444444444
	\item false positives: 762
	\item false negatives: 395
	\item true positives:  4131
	\item true negatives:  3712
	\item precision: 0.8442673206621705
	\item recall: 0.912726469288555
\end{itemize}

\textbf{K = 10:}
\begin{itemize}
	\item Test mean accuracy: 0.812666666667
	\item false positives: 686
	\item false negatives: 1000
	\item true positives:  3468
	\item true negatives:  3846
	\item precision: 0.8348579682233991
	\item recall: 0.7761862130707251
\end{itemize}
\textbf{K = 20:}
\begin{itemize}
	\item Test mean accuracy: 0.794888888889
	\item false positives: 758
	\item false negatives: 1088
	\item true positives:  3444
	\item true negatives:  3710
	\item precision: 0.8196097096620657
	\item recall: 0.7599293909973521
\end{itemize}

Podemos observar que en la medida que aumenta el k, los resultados son peores. Esto se explica debido a que estamos permitiendo votar a más cantidad de 'vecinos' que se encuentran más lejos dentro del hiperplano con respecto a la coordenada en la que se evaluó el mail que queremos clasificar utilizando los parametros dados. Si bien un k muy bajo corre el peligro de que el único criterio de clasificación provenga de un \textit{outlyer}, un k muy alto difumina la idea del método kNN que es, a saber, clasificar nuestro corpus en base a la cercanía respecto al corpus que utilizamos como entrenamiento.



% \subsection{Logistic Regression}



% \subsection{Bagging}
% \subsection{Stacking}

% \section{Reducción de dimensionalidad}

% Describir brevemente las técnicas empleadas. PCA y penalidades.

% \section{Resultados}
% Describir los resultados conseguidos por los distintos modelos y conjuntos de atributos considerados. Preferentemente, resumir los resultados en tablas/figuras. Mencionar los tiempos de ejecución aproximados de cada técnica.


% \section{Discusión}

% Analizar los resultados, buscando responder cuestiones como, por ejemplo: ¿cuáles son los atributos encontrados con mayor poder predictivo?, ¿cuán sensibles fueron los algoritmos a las técnicas de reducción de dimensionalidad consideradas?, ¿resultó clara la elección del algoritmo para la competencia, o hubo que poner en la balanza distintos factores?


\end{document}